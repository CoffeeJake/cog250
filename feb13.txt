=== REVIEW ===

- mircoworld failure
- sterotypes early success into failure
    - repeated issues with prototypes
- cogsci needs to be pursued because 2 domains doesnt work
- GOFI - (good ol fashioned artifical intelligence)
- problem was robot + food source
    - lead to cognatorial explosion
- deeper issue is the relevance problem
- facing a mountian of issues at a deep level
    - perhaps hobbs's central idea was wrong
    - the big issue is that pre-supposition was language like
        - cognition is like an argument like a computer program
        - need to give up language
        - how do explain the relationhsip between cognition and language
        - what would it mean to give up the idea: cognition is language
        - give up theoretical variables
            - instead use "brain like" variables instead of language like
            - cognition is not only "brain like' but "life like"
        - what variables could we do, that nurons fire (1. alter signal strength)
            - 2. valence of the signal (excite / inhibit)
            - 3. weighting of the connection is another variable

=== REVIEW ENDS ===

Can we build strong AI just using these variables?
    - if all you do is use a nural network
        - then we are not proposing an alternative to gofi

2 Opponents to Strong AI
-----------
    1. dealing with descartes
    2. convince hobbs that computation isnt the answer

How to get nural networks running?
    - we have nodes
        - how brainlike does this have to be?
    - networks would have 500/600 nodes
    - all that happens in NNs is that they can only do valence
        - does not include other chemicals (like endocrin)
    - several nodes sending signals to a single node
        - takes into account: signal strength, the valence, and the weighting
        - of EACH connection
            - the idea is you can turn all these into integers and multiplying them
            - weight times signals
            - this is essentially how neural networks work
            - lots of things happening parallel to one another

History on NNs:
    - NNs become important in the early 90s/95s
    - yet they existed many years before this
    - in the 60s they were called perceptrons
        - input nodes vs output nodes
        - complex connected machines
        - mechanized version of behavoirism (psych idea that all things are stimulus -> response)
        - had mathematical proof that they couldnt do (exclusive or)
        - behavorism has been disprooven (formal theory, you can proof that it cant do many important cognative functions)
            - not just something is better, but disprooven
        - hidden layers (not on either input / output sides)
            - thought it wouldnt help
            - well it does
        - additional layers of nodes would not solve the problems
        - change functionality of a network by adding in hidden layers, change its confidence
        - input nodes -> (HIDDEN LAYERS) -> output nodes
        - did i do that? (how do you know what you should do)(what impact your activity has on the network) 
            - come up with a way to assign blame
            - want to manipulate the WEIGHT on the connections in order to assign blame / modify the network

Assigning Blame
    - get a sound wave (using submarine sonar examples)
        - then distinguish if its a rock or mine
        - convert this to a sqeuence of numbers
        - put them into the INPUT nodes, they activate according to numbers
    - then they quickly go through their connections, then the output nodes fire
    - chances are its a wrong answer
    - what it did (performance value) vs what it should have done (target value)
    - take the target value and subtract the performance value
        - gives back an error value
    - mimicing causal processes between the nodes
    - then does statistical analysis to change the weights by assigning blame statistically
    - back propogation of error ^
        - nudge connection weights abit to change the output
        - then try again
    - then you have to give up belief, because its just results from arguments            

Training NNs
    - tank vs no tank training
    - then after training you put it out there, but then it fails
    - nural netowrks pick up on patters in the data
        - whats being offered in place of propositions
        - that pick up in patters in the data
    - pictures were daytime vs nighttime with tanks
    - sampling bias
    - then see if it passes onto test data it hasnt seen before
    - learn complex DeMorgan laws, but not basic things
    - Weak AI / NNs can do lots of cool cool things, just not strong AI
    - in order to do back proposition is the target value (someone has identified it for you)
    - target value comes from someone who has already learned
    - nural networks rely on an external teacher to give it proper data / target values
    - inside the network there is something already LEARNED
    - until the late 90s learning and plasticity (where you could learn but not be molded)
    - yet they could find that you can kill synaptic connections
        - proved false where # of nurons = intelligence
        - synaptogenesis
        - neurongensis - learned the brain has stem cells that can grow new shit
            - even into old age neurogensis exists

    - learning = training networks
    - plasticity = tinkering with the networks
    - the degree to which the network uses suporvised learning is how useless it is
        - cannot satisify the naturalistic imperitive this way
        - unsuporivsed learning


=== Break ===

1996 - unsuporvised learning / plactiscity to be developed
    - language and langague like processing doesn't describe it very well
    - self organized

Unsupurivsed learning = non-humuncular learning
unsuporvised plactisity = non-humuncular plascticity

Geffery Hinton - (correct/brilliant) (but not there yet) 
    - 2 different sets of weights in the same network
        - meaning that 
    - spoken in Phases

Want to get network to recognize letters
    - show the network a bunch of letters
        - all kinds of different "a"s
    - don't correct it, let it just choose
    - then do some data compression
        - make a scatter plot / line of best fit
    - starting to pick up whats invariant though its plot
    - extracts the invariants
        - has a bunch of kernals of invariants
    - network precieves from the world
        - then fanticizes
    - hasnt been given the target value
        - it will pick up on patterns that are not apart of the invariants
    - all variations will contain the invariants
        - its how to look at patterns in the world
        - how to look at complex pattern in the world
    - increasing its ability to detect patterns

    Recognition-Generation Cycle (bootstrap each other up)
        - self organizing
    1. pick up on whats invariant
    2. use that to generate variations
    3. pick up on patterns in the world that will pick up on more complex patterns
    4. better invariants
    5. better variations
    6. better patters
    ETC
    "The Wait-Sleep Algorithm" (official name)

Dreaming - is the generation part of the cycle
    - generating fanticies to deal with the real thing
    - subtle complexities for real patterns
    - bizzare extreme thing, to deal with regular things
        - still new, but plausible things are comming along
        - Unsuporivsed Nural Networks

- Nurons used to recognize emotion are used to generate emotion

Issue with NN, they tend to be single problem solvers    

Unsuporivsed Placticity
    - 1996 - Nershal & Shultz

Casecade Corralation Models
    - introduce nodes, connected to the network such that they can pick up information
        - BUT CANNOT CHANGE THE NETWORKS BEHAVIOR
        - recieve only
    - which activity MOST CLOSLY relates to the error in the network
    - one of the nodes "wins" and then kill off the other
    - now the winning node is now able to interact with other nodes in the network
    - by changing hidden layers, not just amount, but the KIND of functions it can preform

Change in the functions

Formal Systems - completely defined by the logic it uses 
    - to change competence of a system, change its logic

IF you wana change the competence, change its logic
    - and that means its functions will change (for the logic to use)
    - theres no way of running predicate logic, that will generate formal logic
        - cannot generate a stronger logic, from a weaker logic

There is no computational way of going from a weak logic to a strong logic
    - qualitative development























